
# coding: utf-8

# ---
# 
# _You are currently looking at **version 1.2** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-social-network-analysis/resources/yPcBs) course resource._
# 
# ---

# # Assignment 4

# In[1]:

import networkx as nx
import pandas as pd
import numpy as np
import pickle


# ---
# 
# ## Part 1 - Random Graph Identification
# 
# For the first part of this assignment you will analyze randomly generated graphs and determine which algorithm created them.

# In[2]:

P1_Graphs = pickle.load(open('A4_graphs','rb'))
P1_Graphs


# <br>
# `P1_Graphs` is a list containing 5 networkx graphs. Each of these graphs were generated by one of three possible algorithms:
# * Preferential Attachment (`'PA'`)
# * Small World with low probability of rewiring (`'SW_L'`)
# * Small World with high probability of rewiring (`'SW_H'`)
# 
# Anaylze each of the 5 graphs and determine which of the three algorithms generated the graph.
# 
# *The `graph_identification` function should return a list of length 5 where each element in the list is either `'PA'`, `'SW_L'`, or `'SW_H'`.*

# In[10]:

def graph_identification():
    
    clustering = []
    distance = []
    nodes = []
    
    for i in range(5):
        clustering.append(nx.average_clustering(P1_Graphs[i]))
        distance.append(nx.average_shortest_path_length(P1_Graphs[i]))
        nodes.append(len(P1_Graphs[i].nodes()))
    '''
    plt.figure()
    degree = P1_Graphs[0].degree()
    degree_values = sorted(set(degree.values()))
    histogram = [list(degree.values()).count(i)/float(nx.number_of_nodes(P1_Graphs[0])) for i in degree_values]
    plt.bar(degree_values,histogram)
    
    plt.figure()
    degree = P1_Graphs[1].degree()
    degree_values = sorted(set(degree.values()))
    histogram = [list(degree.values()).count(i)/float(nx.number_of_nodes(P1_Graphs[1])) for i in degree_values]
    plt.bar(degree_values,histogram)
    
    plt.figure()
    degree = P1_Graphs[2].degree()
    degree_values = sorted(set(degree.values()))
    histogram = [list(degree.values()).count(i)/float(nx.number_of_nodes(P1_Graphs[2])) for i in degree_values]
    plt.bar(degree_values,histogram)
    
    plt.figure()
    degree = P1_Graphs[3].degree()
    degree_values = sorted(set(degree.values()))
    histogram = [list(degree.values()).count(i)/float(nx.number_of_nodes(P1_Graphs[3])) for i in degree_values]
    plt.bar(degree_values,histogram)
    
    plt.figure()
    degree = P1_Graphs[4].degree()
    degree_values = sorted(set(degree.values()))
    histogram = [list(degree.values()).count(i)/float(nx.number_of_nodes(P1_Graphs[4])) for i in degree_values]
    plt.bar(degree_values,histogram)
    plt.show()
    '''
    return ['PA','SW_H','SW_L','PA','PA']


# ---
# 
# ## Part 2 - Company Emails
# 
# For the second part of this assignment you will be workking with a company's email network where each node corresponds to a person at the company, and each edge indicates that at least one email has been sent between two people.
# 
# The network also contains the node attributes `Department` and `ManagementSalary`.
# 
# `Department` indicates the department in the company which the person belongs to, and `ManagementSalary` indicates whether that person is receiving a management position salary.

# In[3]:

G = nx.read_gpickle('email_prediction.txt')

print(nx.info(G))


# ### Part 2A - Salary Prediction
# 
# Using network `G`, identify the people in the network with missing values for the node attribute `ManagementSalary` and predict whether or not these individuals are receiving a management position salary.
# 
# To accomplish this, you will need to create a matrix of node features using networkx, train a sklearn classifier on nodes that have `ManagementSalary` data, and predict a probability of the node receiving a management salary for nodes where `ManagementSalary` is missing.
# 
# 
# 
# Your predictions will need to be given as the probability that the corresponding employee is receiving a management position salary.
# 
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
# 
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).
# 
# Using your trained classifier, return a series of length 252 with the data being the probability of receiving management salary, and the index being the node id.
# 
#     Example:
#     
#         1       1.0
#         2       0.0
#         5       0.8
#         8       1.0
#             ...
#         996     0.7
#         1000    0.5
#         1001    0.0
#         Length: 252, dtype: float64

# In[20]:

def salary_predictions():
    
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, recall_score, auc, roc_curve, precision_score
    from sklearn.ensemble import GradientBoostingClassifier
    
    df = pd.DataFrame()
    df["degree_cent"] = nx.degree_centrality(G).values()
    df["clustering"] = nx.clustering(G)
    df["closeness"] = nx.closeness_centrality(G,normalized=True).values()
    df["betweenness"] = nx.betweenness_centrality(G,normalized=True,endpoints=False,k=200).values()
    
    dep = [x[1]["Department"] for x in G.nodes(data=True)]
    man_salary = [x[1]["ManagementSalary"] for x in G.nodes(data=True)]
    df["department"] = dep
    df["management salary"] = man_salary
    
    #Separate the data with management salary reported from the rows where no salary is reported
    salary_reported = df.dropna()
    salary_not_reported = df[df["management salary"].isnull()]
    
    x = salary_reported.drop("management salary",axis=1)
    y = salary_reported["management salary"]
    test_df = salary_not_reported.drop("management salary",axis=1)
    
    #Training the gradient boosting model
    X_train,X_test,y_train,y_test = train_test_split(x,y,train_size=0.9,random_state=0) 
    gbm = GradientBoostingClassifier(random_state=0,learning_rate=0.1,n_estimators=45,max_depth=5).fit(X_train,y_train)
    y_score_eval = gbm.decision_function(X_test)
    y_proba_eval = gbm.predict_proba(X_test)
    y_score = gbm.decision_function(test_df)
    y_proba = gbm.predict_proba(test_df)
    
    fpr, tpr, _ = roc_curve(y_test, y_score_eval)
    roc_auc = auc(fpr, tpr)
    
    prob_management_salary = pd.Series(y_proba[:,1])
    prob_management_salary.index = test_df.index
    
    return prob_management_salary


# ### Part 2B - New Connections Prediction
# 
# For the last part of this assignment, you will predict future connections between employees of the network. The future connections information has been loaded into the variable `future_connections`. The index is a tuple indicating a pair of nodes that currently do not have a connection, and the `Future Connection` column indicates if an edge between those two nodes will exist in the future, where a value of 1.0 indicates a future connection.

# In[4]:

future_connections = pd.read_csv('Future_Connections.csv', index_col=0, converters={0: eval})
future_connections.head(10)


# Using network `G` and `future_connections`, identify the edges in `future_connections` with missing values and predict whether or not these edges will have a future connection.
# 
# To accomplish this, you will need to create a matrix of features for the edges found in `future_connections` using networkx, train a sklearn classifier on those edges in `future_connections` that have `Future Connection` data, and predict a probability of the edge being a future connection for those edges in `future_connections` where `Future Connection` is missing.
# 
# 
# 
# Your predictions will need to be given as the probability of the corresponding edge being a future connection.
# 
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
# 
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).
# 
# Using your trained classifier, return a series of length 122112 with the data being the probability of the edge being a future connection, and the index being the edge as represented by a tuple of nodes.
# 
#     Example:
#     
#         (107, 348)    0.35
#         (542, 751)    0.40
#         (20, 426)     0.55
#         (50, 989)     0.35
#                   ...
#         (939, 940)    0.15
#         (555, 905)    0.35
#         (75, 101)     0.65
#         Length: 122112, dtype: float64

# In[9]:

def new_connections_predictions():
    
    import operator
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, recall_score, auc, roc_curve, precision_score
    from sklearn.ensemble import GradientBoostingClassifier
    
    df = future_connections
    
    common_neigh = [(e[0],e[1],len(list(nx.common_neighbors(G,e[0],e[1])))) for e in nx.non_edges(G)]
    common_neigh = sorted(common_neigh,key=operator.itemgetter(0))
    jaccard_coef = list(nx.jaccard_coefficient(G))
    jaccard_coef = sorted(jaccard_coef,key=operator.itemgetter(0))
    resource_alloc = list(nx.resource_allocation_index(G))
    resource_alloc = sorted(resource_alloc,key=operator.itemgetter(0))
    pref_attach = list(nx.preferential_attachment(G))
    pref_attach = sorted(pref_attach,key=operator.itemgetter(0))
    
    df["edge"] = df.index
    df = df.sort_values(by="edge")
    df = df.drop(["edge"],axis=1) #do not understand why these columns were showing up without them being assigned
    
    df["common neighbors"] = list(common_neigh)
    df["common neighbors"] = df["common neighbors"].apply(lambda x: x[2])
    df["jaccard"] = [x[2] for x in jaccard_coef]
    df["resource allocation"] = [x[2] for x in resource_alloc]
    df["preferential attachment"] = [x[2] for x in pref_attach]
    
    #Separate the data with future connection reported from the rows where no data is reported
    conn_data = df.dropna()
    no_conn_data = df[df["Future Connection"].isnull()]
    
    x = conn_data.drop(["Future Connection"],axis=1)
    y = conn_data["Future Connection"]
    test_df = no_conn_data.drop(["Future Connection"],axis=1)
    
    #print (df)
    
    #Training the gradient boosting model
    X_train,X_test,y_train,y_test = train_test_split(x,y,train_size=0.9,random_state=0) 
    gbm = GradientBoostingClassifier(random_state=0,learning_rate=0.1,n_estimators=45,max_depth=5).fit(X_train,y_train)
    y_score_eval = gbm.decision_function(X_test)
    y_proba_eval = gbm.predict_proba(X_test)
    y_score = gbm.decision_function(test_df)
    y_proba = gbm.predict_proba(test_df)
    
    fpr, tpr, _ = roc_curve(y_test, y_score_eval)
    roc_auc = auc(fpr, tpr)
    
    prob_edge = pd.Series(y_proba[:,1])
    prob_edge.index = test_df.index
    
    return prob_edge


# In[ ]:




# In[ ]:




# In[ ]:



